kind: pipeline
name: default

#steps:
#- name: scp
#  image: busybox
#  volumes:
#  - name: share-jars
#    path: /jars
#  commands:
#  - cp ./target/SparkHive-1.0-SNAPSHOT.jar /jars

- name: submit-job
  image: registry.njuics.cn/drone/kubectl
  volumes:
  - name: share-jars
    path: /jars
  commands:
  - cp ./target/SparkHive-1.0-SNAPSHOT.jar /jars
  - rm -rf /root/.kube && cp -r .kube /root
  - kubectl delete -f job.yaml -n kerong || true
  - kubectl delete pod $(kubectl get pods -n kerong --selector=driver-pod=spark-hive --output=jsonpath='{.items[*].metadata.name}') -n kerong
  - kubectl apply -f job.yaml -n kerong
  - sleep 10
  - kubectl logs -f $(kubectl get pods -n kerong --selector=driver-pod=spark-hive --output=jsonpath='{.items[*].metadata.name}') -n kerong
  # - ./bin/spark-submit --master k8s://https://172.16.1.137:6443 --deploy-mode cluster --name spark-hive --class com.wdongyu.hive.SparkHive --conf spark.executor.instances=5 --conf spark.kubernetes.container.image.pullPolicy=Always --conf spark.kubernetes.container.image=registry.njuics.cn/wdongyu/spark:2.4.0 --conf spark.kubernetes.namespace=kerong --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark local:///jars/SparkHive-1.0-SNAPSHOT.jar

volumes:
- name: share-jars
  host:
      path: /jars
